<!doctype html>
<html lang="zh-CN">
    <head>
        <meta charset="ansi">
	<meta name="description" content="this is a website">
        <meta name="keywords" content="website,html,css">

        <title>ReadingGroup</title>

        <link rel="stylesheet" style="text/css" href="index.css">

    </head>
    <body>
        <div id="head">
            <div class="logo_title">
                <h1 style = "text-align:center"><font face="verdana" color="#0D1E91">Deep Robust & Explainable AI Lab Reading Group</font></h1>
          </div>
            <div class="clearfloat"></div>
        </div>

        <div id="wrapper">
            <div class="main">

		<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-07-29</font></h2>
					<h3><a href="https://arxiv.org/abs/2002.12625"><font face="verdana" color="black" size="3"><U>4D Association Graph for Realtime Multi-person Motion Capture Using Multiple Video Cameras</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ruochen Wang</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: This paper contributes a novel realtime multi-person motion capture algorithm using multiview video inputs. Due to the heavy occlusions in each view, joint optimization on the multiview images and multiple temporal frames is indispensable, which brings up the essential challenge of realtime efficiency. To this end, for the first time, we unify per-view parsing, cross-view matching, and temporal tracking into a single optimization framework, i.e., a 4D association graph that each dimension (image space, viewpoint and time) can be treated equally and simultaneously. To solve the 4D association graph efficiently, we further contribute the idea of 4D limb bundle parsing based on heuristic searching, followed with limb bundle assembling by proposing a bundle Kruskal's algorithm. Our method enables a realtime online motion capture system running at 30fps using 5 cameras on a 5-person scene. Benefiting from the unified parsing, matching and tracking constraints, our method is robust to noisy detection, and achieves high-quality online pose reconstruction quality. The proposed method outperforms the state-of-the-art method quantitatively without using high-level appearance information. We also contribute a multiview video dataset synchronized with a marker-based motion capture system for scientific evaluation. </font>
		</p>
	    	</div>
            </div>
		    
		    
		    
	    <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-07-22</font></h2>
					<h3><a href="https://arxiv.org/abs/1908.00598"><font face="verdana" color="black" size="3"><U>Sampling-free Epistemic Uncertainty Estimation Using Approximated Variance Propagation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICCV 2019 (Oral)</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ziyang jia</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: We present a sampling-free approach for computing the epistemic uncertainty of a neural network. Epistemic uncertainty is an important quantity for the deployment of deep neural networks in safety-critical applications, since it represents how much one can trust predictions on new data. Recently promising works were proposed using noise injection combined with Monte-Carlo sampling at inference time to estimate this quantity (e.g. Monte-Carlo dropout). Our main contribution is an approximation of the epistemic uncertainty estimated by these methods that does not require sampling, thus notably reducing the computational overhead. We apply our approach to large-scale visual tasks (i.e., semantic segmentation and depth regression) to demonstrate the advantages of our method compared to sampling-based approaches in terms of quality of the uncertainty estimates as well as of computational overhead. 
		    </font>
		</p>
	    	</div>
            </div>

            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-15</font></h2>
						<h3><a href="https://papers.nips.cc/paper/8777-weight-agnostic-neural-networks"><font face="verdana" color="black" size="3"><U>Weight Agnostic Neural Networks</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: NeurIPS 2019 (Poster)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Zhenzhu Zheng</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: Not all neural network architectures are created equal, some perform much better than others for certain tasks.  But how important are the weight parameters of a neural network compared to its architecture?
                                In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task.
                                We propose a search method for neural network architectures that can already perform a task without any explicit weight training.
                                To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance.
                                We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures
                                that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io
                            </font>
                        </p>
                    </div>
            </div>

            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-08</font></h2>
						<h3><a href="https://openreview.net/forum?id=Bklr3j0cKX"><font face="verdana" color="black" size="3"><U>Deep InfoMax: Learning deep representations by mutual information estimation and maximization</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICLR 2019 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Yi Liu</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals.
                            </font>
                        </p>
                    </div>
            </div>


            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-01</font></h2>
						<h3><a href="https://arxiv.org/pdf/2004.00830.pdf"><font face="verdana" color="black" size="3"><U>Tracking by Instance Detection: A Meta-Learning Approach</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng Ma</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: We consider the tracking problem as a special type of object detection problem, which we call instance detection. With proper initialization, a detector can be quickly converted into a tracker by learning the new instance from a single image. We find that model-agnostic meta-learning (MAML) offers a strategy to initialize the detector that satisfies our needs. We propose a principled three-step approach to build a high-performance tracker. First, pick any modern object detector trained with gradient descent. Second, conduct offline training (or initialization) with MAML. Third, perform domain adaptation using the initial frame. We follow this procedure to build two trackers, named Retina-MAML and FCOS-MAML, based on two modern detectors RetinaNet and FCOS. Evaluations on four benchmarks show that both trackers are competitive against state-of-the-art trackers. On OTB-100, Retina-MAML achieves the highest ever AUC of 0.712. On TrackingNet, FCOS-MAML ranks the first on the leader board with an AUC of 0.757 and the normalized precision of 0.822. Both trackers run in real-time at 40 FPS.
                            </font>
                        </p>
                    </div>
            </div>


            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-06-24</font></h2>
						<h3><a href="https://liuziwei7.github.io/projects/CompoundDomain.html"><font face="verdana" color="black" size="3"><U>Open Compound Domain Adaptation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Fengchun Qiao</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: A typical domain adaptation approach is to adapt models trained on the annotated data in a source domain (e.g., sunny weather) for achieving high performance on the test data in a target domain (e.g., rainy weather). Whether the target contains a single homogeneous domain or multiple heterogeneous domains, existing works always assume that there exist clear distinctions between the domains, which is often not true in practice (e.g., changes in weather). We study an open compound domain adaptation (OCDA) problem, where the target is a compound of multiple homogeneous domains without domain labels, reflecting realistic data collection from mixed and novel situations. We propose a new approach based on two technical insights into OCDA: 1) a curriculum domain adaptation strategy to bootstrap generalization across domains in a data-driven self-organizing fashion and 2) a memory module to increase the model’s agility towards novel domains. Our experiments on digit classification, facial expression recognition, semantic segmentation, and reinforcement learning demonstrate the effectiveness of our approach.
                            </font>
                        </p>
                    </div>
            </div>


            </div>
            <div class="side">
                <div class="author_info">
                    <div class="author_descri">
                        <h4><font face="verdana" color="black" size="4">About Reading Group</font></h4>

                                <p align="left"><font face="verdana" color="black" size="2.8">The reading group is held weekly by <a href="https://sites.google.com/site/xipengcshomepage/research?authuser=0"> <font face="verdana" color="black" size="2.8"> <U> D-REAL </U> </font> </a> at University of Delaware. The goal is to broaden the scope of research interest in <i>Machine Learning</i>, <i>Deep Learning</i>, and <i>Computer Vision</i> by sharing and discussing high-quality papers. </font></p>
                                <br/><p align="left"><font face="verdana" color="black" size="2.8">Each presenter shall provide the link of paper and presentation slides at least ten hours before meeting.</font></p>
                                <br/><p align="left"><font face="verdana" color="black" size="2.8">Please feel free to contact Fengchun (fengchun at udel dot edu) if you want to join the reading group (presentation is highly welcome but not required).</font></p>

                    </div>
                </div>

                <div class="top_article">
                    <h3><font face="verdana" color="black" size="4">Schedule</font></h3>
                    <ul>
                        <li><font face="verdana" color="black" size="2.8">Fengchun: June 24th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Meng: July 1st </font></li>
                        <li><font face="verdana" color="black" size="2.8">Yi: July 8th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Zhenzhu: July 15th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Ziyang: July 22nd </font></li>
                        <li><font face="verdana" color="black" size="2.8">Ruochen: July 29th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Pranjal: August 5th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Yujie : August 12th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Hamed : August 19th </font></li>
                    </ul>
                </div>

                <div class="top_article">
                    <h3><font face="verdana" color="black" size="4">Contact</font></h3>
                    <ul>
                        <li><font face="verdana" color="black" size="2.8">Person: Fengchun Qiao</font></li>
						<li><font face="verdana" color="black" size="2.8">Email: fengchun at udel dot edu</font>
                    </ul>
                </div>

                <div class="top_article">
                    <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=v5s2FYX0VY2D6mH83Q6FIVJ3kgFtRY0IogxL89fjLAg&cl=ffffff&w=a"></script>
                </div>

            </div>

            <div class="clearfloat"></div>
        </div>

    </body>
</html>
