<!doctype html>
<html lang="zh-CN">
    <head>
        <meta charset="ansi">
	<meta name="description" content="this is a website">
        <meta name="keywords" content="website,html,css">

        <title>ReadingGroup</title>

        <link rel="stylesheet" style="text/css" href="index.css">

    </head>
    <body>
        <div id="head">
            <div class="logo_title">
                <h1 style = "text-align:center"><font face="verdana" color="#0D1E91">Deep Robust & Explainable AI Lab Reading Group</font></h1>
          </div>
            <div class="clearfloat"></div>
        </div>

        <div id="wrapper">
            <div class="main">

	  <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-26</font></h2>
					<h3><a href="https://papers.nips.cc/paper/8346-cpm-nets-cross-partial-multi-view-networks"><font face="verdana" color="black" size="3"><U>CPM-Nets: Cross Partial Multi-View Networks</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: NeurIPS 2019 (Poster) </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng Ma</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: Despite multi-view learning progressed fast in past decades, it is still challenging due to the difficulty in modeling complex correlation among different views, especially under the context of view missing. To address the challenge, we propose a novel framework termed Cross Partial Multi-View Networks (CPM-Nets). In this framework, we first give a formal definition of completeness and versatility for multi-view representation and then theoretically prove the versatility of the latent representation learned from our algorithm. To achieve the completeness, the task of learning latent multi-view representation is specifically translated to degradation process through mimicking data transmitting, such that the optimal tradeoff between consistence and complementarity across different views could be achieved. In contrast with methods that either complete missing views or group samples according to view-missing patterns, our model fully exploits all samples and all views to produce structured representation for interpretability. Extensive experimental results validate the effectiveness of our algorithm over existing state-of-the-arts.</div>
		    
	 <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-19</font></h2>
					<h3><a href="https://arxiv.org/abs/2007.15553"><font face="verdana" color="black" size="3"><U>Bilevel Continual Learning</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ArXiv 2020 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Fengchun Qiao</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: Continual learning aims to learn continuously from a stream of tasks and data in an online-learning fashion, being capable of exploiting what was learned previously to improve current and future tasks while still being able to perform well on the previous tasks. One common limitation of many existing continual learning methods is that they often train a model directly on all available training data without validation due to the nature of continual learning, thus suffering poor generalization at test time. In this work, we present a novel framework of continual learning named "Bilevel Continual Learning" (BCL) by unifying a {\it bilevel optimization} objective and a {\it dual memory management} strategy comprising both episodic memory and generalization memory to achieve effective knowledge transfer to future tasks and alleviate catastrophic forgetting on old tasks simultaneously. Our extensive experiments on continual learning benchmarks demonstrate the efficacy of the proposed BCL compared to many state-of-the-art methods. Our implementation is available at https://github.com/phquang/bilevel-continual-learning. </div>
            </div>
		    
	  <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-12</font></h2>
					<h3><a href="https://www.hindawi.com/journals/wcmc/2017/9474806/"><font face="verdana" color="black" size="3"><U>Vision-Based Fall Detection with Convolutional Neural Networks</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: Wireless Communications and Mobile Computing 2017 </font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Hamed Fayyaz</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: One of the biggest challenges in modern societies is the improvement of healthy aging and the support to older persons in their daily activities. In particular, given its social and economic impact, the automatic detection of falls has attracted considerable attention in the computer vision and pattern recognition communities. Although the approaches based on wearable sensors have provided high detection rates, some of the potential users are reluctant to wear them and thus their use is not yet normalized. As a consequence, alternative approaches such as vision-based methods have emerged. We firmly believe that the irruption of the Smart Environments and the Internet of Things paradigms, together with the increasing number of cameras in our daily environment, forms an optimal context for vision-based systems. Consequently, here we propose a vision-based solution using Convolutional Neural Networks to decide if a sequence of frames contains a person falling. To model the video motion and make the system scenario independent, we use optical flow images as input to the networks followed by a novel three-step training phase. Furthermore, our method is evaluated in three public datasets achieving the state-of-the-art results in all three of them.</p>
	    	</div>
            </div>
		    
	   <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-08-05</font></h2>
					<h3><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Mohamed_Social-STGCNN_A_Social_Spatio-Temporal_Graph_Convolutional_Neural_Network_for_Human_CVPR_2020_paper.html"><font face="verdana" color="black" size="3"><U>Social-STGCNN: A Social Spatio-Temporal Graph Convolutional Neural Network for Human Trajectory Prediction</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Poster)</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Pranjal Dhakal</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: Better machine understanding of pedestrian behaviors enables faster progress in modeling interactions between agents such as autonomous vehicles and humans. Pedestrian trajectories are not only influenced by the pedestrian itself but also by interaction with surrounding objects. Previous methods modeled these interactions by using a variety of aggregation methods that integrate different learned pedestrians states. We propose the Social Spatio-Temporal Graph Convolutional Neural Network (Social-STGCNN), which substitutes the need of aggregation methods by modeling the interactions as a graph. Our results show an improvement over the state of art by 20% on the Final Displacement Error (FDE) and an improvement on the Average Displacement Error (ADE) with 8.5 times less parameters and up to 48 times faster inference speed than previously reported methods. In addition, our model is data efficient, and exceeds previous state of the art on the ADE metric with only 20% of the training data. We propose a kernel function to embed the social interactions between pedestrians within the adjacency matrix. Through qualitative analysis, we show that our model inherited social behaviors that can be expected between pedestrians trajectories. Code is available at https://github.com/abduallahmohamed/Social-STGCNN. 
		</p>
	    	</div>
            </div>
		    
		    
		    
		<div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-07-29</font></h2>
					<h3><a href="https://arxiv.org/abs/2002.12625"><font face="verdana" color="black" size="3"><U>4D Association Graph for Realtime Multi-person Motion Capture Using Multiple Video Cameras</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ruochen Wang</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: This paper contributes a novel realtime multi-person motion capture algorithm using multiview video inputs. Due to the heavy occlusions in each view, joint optimization on the multiview images and multiple temporal frames is indispensable, which brings up the essential challenge of realtime efficiency. To this end, for the first time, we unify per-view parsing, cross-view matching, and temporal tracking into a single optimization framework, i.e., a 4D association graph that each dimension (image space, viewpoint and time) can be treated equally and simultaneously. To solve the 4D association graph efficiently, we further contribute the idea of 4D limb bundle parsing based on heuristic searching, followed with limb bundle assembling by proposing a bundle Kruskal's algorithm. Our method enables a realtime online motion capture system running at 30fps using 5 cameras on a 5-person scene. Benefiting from the unified parsing, matching and tracking constraints, our method is robust to noisy detection, and achieves high-quality online pose reconstruction quality. The proposed method outperforms the state-of-the-art method quantitatively without using high-level appearance information. We also contribute a multiview video dataset synchronized with a marker-based motion capture system for scientific evaluation. </font>
		</p>
	    	</div>
            </div>
		    
		    
		    
	    <div class="item">
	    	<div class="item_content">
					<h2><font face="verdana" color="black" size="5">2020-07-22</font></h2>
					<h3><a href="https://arxiv.org/abs/1908.00598"><font face="verdana" color="black" size="3"><U>Sampling-free Epistemic Uncertainty Estimation Using Approximated Variance Propagation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
					<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICCV 2019 (Oral)</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Ziyang jia</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
		<p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
		<p class="item_descri">
		    <font face="verdana" color="black" size="2.5">
		    Abstract: We present a sampling-free approach for computing the epistemic uncertainty of a neural network. Epistemic uncertainty is an important quantity for the deployment of deep neural networks in safety-critical applications, since it represents how much one can trust predictions on new data. Recently promising works were proposed using noise injection combined with Monte-Carlo sampling at inference time to estimate this quantity (e.g. Monte-Carlo dropout). Our main contribution is an approximation of the epistemic uncertainty estimated by these methods that does not require sampling, thus notably reducing the computational overhead. We apply our approach to large-scale visual tasks (i.e., semantic segmentation and depth regression) to demonstrate the advantages of our method compared to sampling-based approaches in terms of quality of the uncertainty estimates as well as of computational overhead. 
		    </font>
		</p>
	    	</div>
            </div>

            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-15</font></h2>
						<h3><a href="https://papers.nips.cc/paper/8777-weight-agnostic-neural-networks"><font face="verdana" color="black" size="3"><U>Weight Agnostic Neural Networks</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: NeurIPS 2019 (Poster)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Zhenzhu Zheng</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: Not all neural network architectures are created equal, some perform much better than others for certain tasks.  But how important are the weight parameters of a neural network compared to its architecture?
                                In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task.
                                We propose a search method for neural network architectures that can already perform a task without any explicit weight training.
                                To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance.
                                We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures
                                that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io
                            </font>
                        </p>
                    </div>
            </div>

            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-08</font></h2>
						<h3><a href="https://openreview.net/forum?id=Bklr3j0cKX"><font face="verdana" color="black" size="3"><U>Deep InfoMax: Learning deep representations by mutual information estimation and maximization</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: ICLR 2019 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Yi Liu</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: This work investigates unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality in the input into the objective can significantly improve a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and compares favorably with fully-supervised learning on several classification tasks in with some standard architectures. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation learning objectives for specific end-goals.
                            </font>
                        </p>
                    </div>
            </div>


            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-07-01</font></h2>
						<h3><a href="https://arxiv.org/pdf/2004.00830.pdf"><font face="verdana" color="black" size="3"><U>Tracking by Instance Detection: A Meta-Learning Approach</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Meng Ma</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: We consider the tracking problem as a special type of object detection problem, which we call instance detection. With proper initialization, a detector can be quickly converted into a tracker by learning the new instance from a single image. We find that model-agnostic meta-learning (MAML) offers a strategy to initialize the detector that satisfies our needs. We propose a principled three-step approach to build a high-performance tracker. First, pick any modern object detector trained with gradient descent. Second, conduct offline training (or initialization) with MAML. Third, perform domain adaptation using the initial frame. We follow this procedure to build two trackers, named Retina-MAML and FCOS-MAML, based on two modern detectors RetinaNet and FCOS. Evaluations on four benchmarks show that both trackers are competitive against state-of-the-art trackers. On OTB-100, Retina-MAML achieves the highest ever AUC of 0.712. On TrackingNet, FCOS-MAML ranks the first on the leader board with an AUC of 0.757 and the normalized precision of 0.822. Both trackers run in real-time at 40 FPS.
                            </font>
                        </p>
                    </div>
            </div>


            <div class="item">
                    <div class="item_content">
						<h2><font face="verdana" color="black" size="5">2020-06-24</font></h2>
						<h3><a href="https://liuziwei7.github.io/projects/CompoundDomain.html"><font face="verdana" color="black" size="3"><U>Open Compound Domain Adaptation</U><font face="verdana" color="black" size="0.5"></font> </font></a></h3>
						<p class="item_info"><font face="verdana" color="black" size="2.5">Accepted to: CVPR 2020 (Oral)</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Presenter : Fengchun Qiao</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Time: 9:00-10:00 p.m. EST</font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Zoom: <a href="https://udel.zoom.us/j/96483565680"><font face="verdana" color="red" size="2.5">https://udel.zoom.us/j/96483565680</font></a></font></p>
                        <p class="item_info"><font face="verdana" color="black" size="2.5">Slides: <a href="https://docs.google.com/presentation/d/1senfVLamhInuB7S4LYhgmswXmF3iPS5Ac5RDiilsHIs/edit?usp=sharing"><font face="verdana" color="red" size="2.5">link</font></a></font></p>
                        <p class="item_descri">
                            <font face="verdana" color="black" size="2.5">
                            Abstract: A typical domain adaptation approach is to adapt models trained on the annotated data in a source domain (e.g., sunny weather) for achieving high performance on the test data in a target domain (e.g., rainy weather). Whether the target contains a single homogeneous domain or multiple heterogeneous domains, existing works always assume that there exist clear distinctions between the domains, which is often not true in practice (e.g., changes in weather). We study an open compound domain adaptation (OCDA) problem, where the target is a compound of multiple homogeneous domains without domain labels, reflecting realistic data collection from mixed and novel situations. We propose a new approach based on two technical insights into OCDA: 1) a curriculum domain adaptation strategy to bootstrap generalization across domains in a data-driven self-organizing fashion and 2) a memory module to increase the model’s agility towards novel domains. Our experiments on digit classification, facial expression recognition, semantic segmentation, and reinforcement learning demonstrate the effectiveness of our approach.
                            </font>
                        </p>
                    </div>
            </div>


            </div>
            <div class="side">
                <div class="author_info">
                    <div class="author_descri">
                        <h4><font face="verdana" color="black" size="4">About Reading Group</font></h4>

                                <p align="left"><font face="verdana" color="black" size="2.8">The reading group is held weekly by <a href="https://sites.google.com/site/xipengcshomepage/research?authuser=0"> <font face="verdana" color="black" size="2.8"> <U> D-REAL </U> </font> </a> at University of Delaware. The goal is to broaden the scope of research interest in <i>Machine Learning</i>, <i>Deep Learning</i>, and <i>Computer Vision</i> by sharing and discussing high-quality papers. </font></p>
                                <br/><p align="left"><font face="verdana" color="black" size="2.8">Each presenter shall provide the link of paper and presentation slides at least ten hours before meeting.</font></p>
                                <br/><p align="left"><font face="verdana" color="black" size="2.8">Please feel free to contact Fengchun (fengchun at udel dot edu) if you want to join the reading group (presentation is highly welcome but not required).</font></p>

                    </div>
                </div>

                <div class="top_article">
                    <h3><font face="verdana" color="black" size="4">Schedule</font></h3>
                    <ul>
                        <li><font face="verdana" color="black" size="2.8">Fengchun: August 19th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Meng: August 26th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Yi: September 2nd </font></li>
                        <li><font face="verdana" color="black" size="2.8">Zhenzhu: September 9nd </font></li>
                        <li><font face="verdana" color="black" size="2.8">Ziyang: September 16nd </font></li>
                        <li><font face="verdana" color="black" size="2.8">Ruochen: September 23rd </font></li>
                        <li><font face="verdana" color="black" size="2.8">Pranjal: September 30th </font></li>
                        <li><font face="verdana" color="black" size="2.8">Hamed : October 7th </font></li>
                    </ul>
                </div>

                <div class="top_article">
                    <h3><font face="verdana" color="black" size="4">Contact</font></h3>
                    <ul>
                        <li><font face="verdana" color="black" size="2.8">Person: Fengchun Qiao</font></li>
						<li><font face="verdana" color="black" size="2.8">Email: fengchun at udel dot edu</font>
                    </ul>
                </div>

                <div class="top_article">
                    <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=v5s2FYX0VY2D6mH83Q6FIVJ3kgFtRY0IogxL89fjLAg&cl=ffffff&w=a"></script>
                </div>

            </div>

            <div class="clearfloat"></div>
        </div>

    </body>
</html>
